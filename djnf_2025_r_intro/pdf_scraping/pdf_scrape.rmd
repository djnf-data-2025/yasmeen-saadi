---
title: "scraping_data_from_pdfs with R"
format: html
editor: visual
---

# Scraping data from PDFs the hard way and the easy way

## The Hard Way (Only here to scare you)

### Load libraries

```{r}

#
# if not installed
#

if (!requireNamespace("pdftools", quietly = TRUE)) {
  install.packages("pdftools")
}
# if not installed tidyverse
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}

# if not installed
if (!requireNamespace("janitor", quietly = TRUE)) {
  install.packages("janitor")
}

#
# Load libraries
#

library(pdftools)
library(tidyverse)
library(janitor)


```

### Read in a PDF

```{r}
# Read in a PDF
bp_fy2020_filepath <- "fy/fy_2020_bp.pdf"

bp_fy2020 <- pdf_text(bp_fy2020_filepath)

bp_fy2020
```

### Write Regex to Parse it into a table (HAHAHAHA, Look at this nonsense)

```{r}

# (1): ask ChatGPT what to do! 

# (2) Build a regex with one capture group for each column:
pattern <- paste0(
  "^\\s*", 
  "(.+?)",                         # 1: SECTOR (lazy: grab as few chars as possible until spacing to numbers)
  "\\s+", 
  "([0-9,]+)\\s+",                 # 2: October
  "([0-9,]+)\\s+",                 # 3: November
  "([0-9,]+)\\s+",                 # 4: December
  "([0-9,]+)\\s+",                 # 5: January
  "([0-9,]+)\\s+",                 # 6: February
  "([0-9,]+)\\s+",                 # 7: March
  "([0-9,]+)\\s+",                 # 8: April
  "([0-9,]+)\\s+",                 # 9: May
  "([0-9,]+)\\s+",                 # 10: June
  "([0-9,]+)\\s+",                 # 11: July
  "([0-9,]+)\\s+",                 # 12: August
  "([0-9,]+)\\s+",                 # 13: September
  "([0-9,]+)\\s*$"                 # 14: Yearly Total
)

# (3) Split the raw text into lines, skip the header, then apply str_match:
lines <- str_split(bp_fy2020, "\n")[[1]]

# Drop any blank lines or the header line that begins with "SECTOR"
data_lines <- lines[grepl("\\S", lines) & !grepl("^\\s*SECTOR", lines)]

# (4) Extract all columns into a matrix: each row → 14 columns
m <- str_match(data_lines, pattern)

# The first column of m is the full match; columns 2:15 are our captures
bp_fy2020_df <- data.frame(
  Sector       = str_trim(m[,2]),
  October      = as.integer(gsub(",", "", m[,3])),
  November     = as.integer(gsub(",", "", m[,4])),
  December     = as.integer(gsub(",", "", m[,5])),
  January      = as.integer(gsub(",", "", m[,6])),
  February     = as.integer(gsub(",", "", m[,7])),
  March        = as.integer(gsub(",", "", m[,8])),
  April        = as.integer(gsub(",", "", m[,9])),
  May          = as.integer(gsub(",", "", m[,10])),
  June         = as.integer(gsub(",", "", m[,11])),
  July         = as.integer(gsub(",", "", m[,12])),
  August       = as.integer(gsub(",", "", m[,13])),
  September    = as.integer(gsub(",", "", m[,14])),
  YearlyTotal  = as.integer(gsub(",", "", m[,15])),
  stringsAsFactors = FALSE
) %>%
  filter(!is.na(Sector)) %>%
  clean_names() 

# Inspect the resulting data.frame
bp_fy2020_df 

```

## The Easy Way (Help Me, Mr. LLM)

### Load libraries

```{r}
# if not installed
if (!requireNamespace("ellmer", quietly = TRUE)) {
  install.packages("ellmer")
}

# https://ellmer.tidyverse.org/
library(ellmer)

```

### Setup Google API Key

```{r}
# This is mine, you need to get your own!
# https://aistudio.google.com/prompts/new_chat
Sys.setenv(GOOGLE_API_KEY = "AIzaSyCciy0_2az6IY-UpW2jW9PmHKJFtsFi5fY")

```

### Make sure it's working

```{r}
#Start a Gemini chat session
chat <- chat_google_gemini(
  model  = "gemini-2.5-flash-preview-05-20"
)

# Ask it a question! 
response <- chat$chat("How are you feeling today, Gemini?")

```

### Get data from one PDF

```{r}
#Define a “string-only” schema with exactly your columns

table_schema <- type_array(
  items = type_object(
    SECTOR         = type_string("e.g. 'Southwest Border'"),
    October        = type_string("e.g. '35,402'"),
    November       = type_string("e.g. '33,524'"),
    December       = type_string("e.g. '32,853'"),
    January        = type_string("e.g. '29,205'"),
    February       = type_string("e.g. '30,077'"),
    March          = type_string("e.g. '30,389'"),
    April          = type_string("e.g. '16,182'"),
    May            = type_string("e.g. '21,593'"),
    June           = type_string("e.g. '30,836'"),
    July           = type_string("e.g. '38,536'"),
    August         = type_string("e.g. '47,283'"),
    September      = type_string("e.g. '54,771'"),
    `Yearly Total` = type_string("e.g. '400,651'")
  )
)

# Upload a PDF to Gemini
bp_fy2020_filepath <- "fy/fy_2020_bp.pdf"
bp_fy2020_file <- google_upload(path = bp_fy2020_filepath)


# Start a Gemini chat session
chat <- chat_google_gemini(
  model = "gemini-2.5-flash-preview-05-20"
)

# Ask it to parse data into a table

bp_fy2020 <- chat$chat_structured(
  bp_fy2020_file,
  type = table_schema
) 

# Clean it up
bp_fy2020_df <- bp_fy2020 |>
  mutate(filename = bp_fy2020_filepath) |>
  clean_names() |>
  select(filename, everything()) |>
  mutate(across(c(october:yearly_total),
                 ~ parse_number(.x)))

bp_fy2020_df

```

### Get data from many PDFs sequentially

```{r}

#
# Define a function 
#

parse_bp_pdf <- function(file_path) {
  
  #Upload the PDF that contains your table

  file_content <- google_upload(path = file_path)
  
  # Create a new chat
  chat <- chat_google_gemini(
  model = "gemini-2.5-flash-preview-05-20")
  
  # Get a response
  table_data <- chat$chat_structured(
  file_content,
  type = table_schema)
  
  # Clean and format the data
  table_data <- table_data %>%
    mutate(filename = file_path) %>%
    clean_names() %>%
    select(filename, everything()) 

  return(table_data)
}

#
# Make a list of PDFs
#

# Make a list of all filepaths in fy folder
file_paths <- list.files("fy", full.names = TRUE, pattern = "\\.pdf$")
# For testing, limit to the first few files
file_paths <- file_paths[1:2] # For testing, just use the first file

#
# Apply our function to our file list to create one dataframe
#

# Now your future_map_dfr will run on (total_cores - 1) background R sessions:
bp_fy_all <- map_dfr(
  file_paths,
  parse_bp_pdf,
  .progress = TRUE
)

# Clean up the final dataframe
bp_fy_all_df <- bp_fy_all |>
  mutate(across(c(october:yearly_total),
                 ~ parse_number(.x)))

bp_fy_all_df
```

### Get data from many PDFs in paralell (fastest way)

```{r}

#
# Load libraries
#

# if not installed
if (!requireNamespace("furrr", quietly = TRUE)) {
  install.packages("furrr")
}

# if not installed
if (!requireNamespace("parallel", quietly = TRUE)) {
  install.packages("parallel")
}

# if not installed
if (!requireNamespace("future", quietly = TRUE)) {
  install.packages("future")
}

library(future)
library(furrr)
library(parallel)  


#
# Define a function 
#

parse_bp_pdf <- function(file_path) {
  
  #Upload the PDF that contains your table

  file_content <- google_upload(path = file_path)
  
  # Create a new chat
  chat <- chat_google_gemini(
  model = "gemini-2.5-flash-preview-05-20")
  
  # Get a response
  table_data <- chat$chat_structured(
  file_content,
  type = table_schema)
  
  # Clean and format the data
  table_data <- table_data %>%
    mutate(filename = file_path) %>%
    clean_names() %>%
    select(filename, everything()) 

  return(table_data)
}

#
# Make a list of PDFs
#

# Make a list of all filepaths in fy folder
file_paths <- list.files("fy", full.names = TRUE, pattern = "\\.pdf$")
# For testing, limit to the first few files
file_paths <- file_paths[1:16] # For testing, just use the first file

# Figure out how many cores you have, and subtract 1 (but at least 1 worker)
total_cores <- detectCores(logical = TRUE)
n_workers   <- max(total_cores - 1, 1)

# Use a multisession plan with one fewer than the total cores
plan(multisession, workers = n_workers)

# Now your future_map_dfr will run on (total_cores - 1) background R sessions:
bp_fy_all <- future_map_dfr(
  file_paths,
  parse_bp_pdf,
  .progress = TRUE
)

# Clean up the final dataframe
bp_fy_all_df <- bp_fy_all |>
  mutate(across(c(october:yearly_total),
                 ~ parse_number(.x)))

bp_fy_all_df
```
